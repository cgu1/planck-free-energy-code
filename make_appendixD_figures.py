"""Appendix D figure generator (Poisson-cluster / cascade time-tag model).

This is a self-contained script: it does NOT require any local package structure.
It writes three PDFs to an output folder (default: figs/):
  - appendixD_g2_tau.pdf
  - appendixD_g2_vs_dt.pdf
  - appendixD_fano_vs_dt.pdf

Model (Appendix D):
  Kick times S_i form a Poisson process of rate r.
  Each kick i produces a burst size N_i ~ Geom on {0,1,2,...} with
      P(N=n) = (1-q) q^n,  q = exp(-gamma).
  Conditional on N_i, emission delays D_{ij} are i.i.d. Exp(kappa).
  Photon time tags are t_{ij} = S_i + D_{ij}.
  (Cascades overlap naturally because different kicks superpose.)

We simulate on a finite observation window [0, T]. To approximate stationarity,
we also simulate kicks on a padded interval [-T_pad, T] and keep only photons
that land in [0, T].

Baseline:
  A Poisson process of photons on [0, T] with rate nu = r * <N> = r/(exp(gamma)-1).

Diagnostics:
  From binned counts n_j in bins of width Δt:
    g2(kΔt) = <n_j n_{j+k}> / <n_j>^2,   k>=1
    g2_Δt(0) = <n_j(n_j-1)>/<n_j>^2
    F(Δt) = Var(n_j)/<n_j>

Analytic curves (Appendix D):
  g2(τ) = 1 + (kappa/r) exp(-kappa |τ|)
  g2_Δt(0) = 1 + (2/r) [Δt - (1-exp(-kappa Δt))/kappa] / Δt^2
  F(Δt) = 1 + (2 nu / r) [1 - (1-exp(-kappa Δt))/(kappa Δt)]

Run:
  python make_appendixD_figures.py --outdir figs

Author: (generated by ChatGPT for the paper workflow)
"""

from __future__ import annotations

import argparse
import math
import os
from dataclasses import dataclass
from typing import Tuple

import numpy as np
import matplotlib.pyplot as plt


# -------------------------
# Analytic formulas
# -------------------------

def analytic_g2_tau(tau: np.ndarray, r: float, kappa: float) -> np.ndarray:
    tau = np.asarray(tau, dtype=float)
    return 1.0 + (kappa / r) * np.exp(-kappa * np.abs(tau))


def analytic_g2_dt0(dt: np.ndarray, r: float, kappa: float) -> np.ndarray:
    """Eq. (g2b0_cluster): binned g2_Δt(0) for the cluster model."""
    dt = np.asarray(dt, dtype=float)
    # Avoid division by 0 at dt=0
    out = np.empty_like(dt)
    small = dt <= 0
    out[small] = np.nan

    d = dt[~small]
    term = d - (1.0 - np.exp(-kappa * d)) / kappa
    out[~small] = 1.0 + (2.0 / r) * term / (d * d)
    return out


def analytic_fano_dt(dt: np.ndarray, r: float, kappa: float, nu: float) -> np.ndarray:
    """Eq. (fano_cluster): binned Fano factor for the cluster model."""
    dt = np.asarray(dt, dtype=float)
    out = np.empty_like(dt)
    small = dt <= 0
    out[small] = np.nan

    d = dt[~small]
    out[~small] = 1.0 + (2.0 * nu / r) * (1.0 - (1.0 - np.exp(-kappa * d)) / (kappa * d))
    return out


# -------------------------
# Simulation
# -------------------------

@dataclass(frozen=True)
class SimParams:
    r: float
    kappa: float
    gamma: float
    T: float
    pad_mult: float
    seed: int


def _validate_params(p: SimParams) -> None:
    if p.r <= 0:
        raise ValueError("r must be positive")
    if p.kappa <= 0:
        raise ValueError("kappa must be positive")
    if p.T <= 0:
        raise ValueError("T must be positive")
    if p.pad_mult < 0:
        raise ValueError("pad_mult must be nonnegative")


def simulate_cluster_times(p: SimParams) -> Tuple[np.ndarray, float]:
    """Simulate cluster photon times in [0, T].

    Returns:
      times: sorted photon time tags within [0, T]
      nu_theory: theoretical mean photon rate r * <N>

    Notes:
      We generate kicks in an extended window [-T_pad, T] so that photons emitted
      shortly after t=0 from pre-window kicks are present, approximating stationarity.
    """
    _validate_params(p)
    rng = np.random.default_rng(p.seed)

    q = math.exp(-p.gamma)
    # Mean burst size on {0,1,...}: <N> = q/(1-q) = 1/(exp(gamma)-1)
    mean_N = 1.0 / (math.exp(p.gamma) - 1.0)
    nu_theory = p.r * mean_N

    T_pad = (p.pad_mult / p.kappa) if p.pad_mult > 0 else 0.0
    T_ext = p.T + T_pad

    # Number of kicks in the extended window
    C = rng.poisson(lam=p.r * T_ext)
    if C == 0:
        return np.empty(0, dtype=float), nu_theory

    # Kick times: uniform in [-T_pad, T]
    S = rng.uniform(low=-T_pad, high=p.T, size=C)

    # Burst sizes: geometric on {0,1,2,...}
    # numpy.geometric(p) returns support {1,2,...} with mean 1/p
    N = rng.geometric(p=1.0 - q, size=C) - 1

    total_photons = int(N.sum())
    if total_photons == 0:
        return np.empty(0, dtype=float), nu_theory

    # Parent time for each photon
    S_rep = np.repeat(S, N)

    # Independent emission delays
    D = rng.exponential(scale=1.0 / p.kappa, size=total_photons)
    t = S_rep + D

    # Keep only photons in the observation window
    t = t[(t >= 0.0) & (t <= p.T)]
    t.sort()
    return t, nu_theory


def simulate_poisson_times(T: float, nu: float, seed: int) -> np.ndarray:
    """Simulate Poisson photon times in [0, T] with rate nu."""
    if nu <= 0:
        return np.empty(0, dtype=float)
    rng = np.random.default_rng(seed)
    N = rng.poisson(lam=nu * T)
    if N == 0:
        return np.empty(0, dtype=float)
    t = rng.uniform(low=0.0, high=T, size=N)
    t.sort()
    return t


# -------------------------
# Binning and estimators
# -------------------------


def bin_counts(times: np.ndarray, dt: float, T: float) -> np.ndarray:
    """Bin event times into counts in [0,T) with bin width dt."""
    if dt <= 0:
        raise ValueError("dt must be positive")
    nbins = int(math.floor(T / dt))
    if nbins <= 0:
        raise ValueError("dt too large for the chosen T")
    # Map times to bin indices
    idx = (times / dt).astype(np.int64)
    idx = idx[(idx >= 0) & (idx < nbins)]
    return np.bincount(idx, minlength=nbins)


def estimate_g2_0_and_fano(counts: np.ndarray) -> Tuple[float, float, float]:
    """Return (mean_n, g2_dt0, fano)."""
    m = counts.mean()
    if m <= 0:
        return m, float("nan"), float("nan")

    g2_0 = (counts * (counts - 1)).mean() / (m * m)
    fano = counts.var(ddof=0) / m
    return m, g2_0, fano


def estimate_g2_tau(counts: np.ndarray, max_lag: int) -> Tuple[np.ndarray, np.ndarray]:
    """Estimate g2(k) for k=1..max_lag from binned counts using FFT autocorrelation."""
    if max_lag < 1:
        raise ValueError("max_lag must be >= 1")
    x = counts.astype(float)
    m = x.mean()
    if m <= 0:
        k = np.arange(1, max_lag + 1)
        return k, np.full_like(k, np.nan, dtype=float)

    M = x.size
    # Zero-pad to avoid circular correlation
    nfft = 1 << int(math.ceil(math.log2(2 * M)))
    fx = np.fft.rfft(x, n=nfft)
    ac = np.fft.irfft(fx * np.conj(fx), n=nfft)[: M]  # sums_{j} x_j x_{j+k}

    # Convert sums to averages over valid j (M-k samples)
    ks = np.arange(1, max_lag + 1)
    denom = (M - ks).astype(float)
    mean_prod = ac[ks] / denom
    g2 = mean_prod / (m * m)
    return ks, g2


# -------------------------
# Plotting
# -------------------------


def _savefig(path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    plt.tight_layout()
    plt.savefig(path)
    plt.close()


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--outdir", default="figs", help="Output folder for PDFs (default: figs)")
    ap.add_argument("--seed", type=int, default=123, help="RNG seed")

    # Model parameters
    ap.add_argument("--kappa", type=float, default=1.0, help="Exponential delay rate κ")
    ap.add_argument("--r", type=float, default=1.0, help="Kick rate r")
    ap.add_argument("--gamma", type=float, default=1.0, help="γ = ħω/(k_B T)")

    # Simulation size
    ap.add_argument(
        "--target_cascades",
        type=int,
        default=1000_000,
        help="Target expected number of cascades in the observation window",
    )
    ap.add_argument(
        "--pad_mult",
        type=float,
        default=10.0,
        help="Padding in units of 1/κ (simulate kicks on [-pad_mult/κ, T])",
    )

    # g2(tau) plot controls
    ap.add_argument("--dt_tau", type=float, default=0.05, help="Bin width Δt for g2(τ) plot")
    ap.add_argument("--max_tau", type=float, default=2.0, help="Max τ shown in g2(τ) plot")

    # Δt sweep controls
    ap.add_argument(
        "--dt_min", type=float, default=0.02, help="Minimum Δt in Δt-sweep (in units of 1/κ)")
    ap.add_argument(
        "--dt_max", type=float, default=20.0, help="Maximum Δt in Δt-sweep (in units of 1/κ)")
    ap.add_argument("--n_dt", type=int, default=24, help="Number of Δt points")

    args = ap.parse_args()

    # Interpret dt settings in absolute time units: user passes multiples of 1/κ for sweeps
    kappa = float(args.kappa)
    r = float(args.r)
    gamma = float(args.gamma)

    # Choose T so that E[C] = r T = target_cascades
    T = float(args.target_cascades) / r

    p = SimParams(r=r, kappa=kappa, gamma=gamma, T=T, pad_mult=float(args.pad_mult), seed=int(args.seed))

    # Simulate cluster and Poisson baseline
    cluster_times, nu_theory = simulate_cluster_times(p)
    poisson_times = simulate_poisson_times(T=T, nu=nu_theory, seed=int(args.seed) + 1)

    print(f"[Appendix D] Simulated window T = {T:.3g}")
    print(f"[Appendix D] Cluster photons kept in [0,T]: {cluster_times.size}")
    print(f"[Appendix D] Poisson photons in [0,T]:       {poisson_times.size}")
    print(f"[Appendix D] Theoretical mean rate nu = {nu_theory:.6g}")

    outdir = args.outdir
    os.makedirs(outdir, exist_ok=True)

    # ---------------- g2(tau) ----------------
    dt_tau = float(args.dt_tau) / kappa  # interpret as multiple of 1/κ
    max_tau = float(args.max_tau) / kappa
    max_lag = int(math.floor(max_tau / dt_tau))
    max_lag = max(1, max_lag)

    counts_c = bin_counts(cluster_times, dt_tau, T)
    counts_p = bin_counts(poisson_times, dt_tau, T)

    ks, g2_c = estimate_g2_tau(counts_c, max_lag=max_lag)
    _, g2_p = estimate_g2_tau(counts_p, max_lag=max_lag)

    tau = ks * dt_tau
    g2_analytic = analytic_g2_tau(tau, r=r, kappa=kappa)

    plt.figure()
    plt.plot(tau, g2_c, marker="o", label="Cluster (sim)")
    plt.plot(tau, g2_p, marker="o", linestyle="None", markersize=3, label="Poisson (sim)")
    plt.plot(tau, g2_analytic, linestyle="--", label="Cluster (analytic)")
    plt.axhline(1.0, linestyle=":")
    plt.xlabel(r"$\tau$")
    plt.ylabel(r"$g^{(2)}(\tau)$")
    plt.legend()
    _savefig(os.path.join(outdir, "appendixD_g2_tau.pdf"))

    # ---------------- g2(0) vs dt and Fano vs dt ----------------
    dt_list = np.logspace(np.log10(args.dt_min), np.log10(args.dt_max), int(args.n_dt)) / kappa

    g2_0_c = []
    g2_0_p = []
    fano_c = []
    fano_p = []

    for dt in dt_list:
        c_c = bin_counts(cluster_times, dt, T)
        c_p = bin_counts(poisson_times, dt, T)

        _, g2c, fc = estimate_g2_0_and_fano(c_c)
        _, g2p, fp = estimate_g2_0_and_fano(c_p)

        g2_0_c.append(g2c)
        g2_0_p.append(g2p)
        fano_c.append(fc)
        fano_p.append(fp)

    g2_0_c = np.asarray(g2_0_c)
    g2_0_p = np.asarray(g2_0_p)
    fano_c = np.asarray(fano_c)
    fano_p = np.asarray(fano_p)

    g2_0_analytic = analytic_g2_dt0(dt_list, r=r, kappa=kappa)
    fano_analytic = analytic_fano_dt(dt_list, r=r, kappa=kappa, nu=nu_theory)

    # g2(0) vs dt
    plt.figure()
    plt.plot(dt_list, g2_0_c,marker="o", label="Cluster (sim)")
    plt.plot(dt_list, g2_0_p, marker="o", linestyle="None", markersize=3, label="Poisson (sim)")
    plt.plot(dt_list, g2_0_analytic, linestyle="--", label="Cluster (analytic)")
    plt.axhline(1.0, linestyle=":")
    plt.xscale("log")
    plt.xlabel(r"$\Delta t$")
    plt.ylabel(r"$g^{(2)}_{\Delta t}(0)$")
    plt.legend()
    _savefig(os.path.join(outdir, "appendixD_g2_vs_dt.pdf"))

    # Fano vs dt
    plt.figure()
    plt.plot(dt_list, fano_c,marker="o", label="Cluster (sim)")
    plt.plot(dt_list, fano_p, marker="o", linestyle="None", markersize=3, label="Poisson (sim)")
    plt.plot(dt_list, fano_analytic, linestyle="--", label="Cluster (analytic)")
    plt.axhline(1.0, linestyle=":")
    plt.xscale("log")
    plt.xlabel(r"$\Delta t$")
    plt.ylabel(r"$F(\Delta t)$")
    plt.legend()
    _savefig(os.path.join(outdir, "appendixD_fano_vs_dt.pdf"))

    print(f"[Appendix D] Wrote PDFs to: {os.path.abspath(outdir)}")


if __name__ == "__main__":
    main()
